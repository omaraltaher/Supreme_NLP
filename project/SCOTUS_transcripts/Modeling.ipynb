{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into dev and training sets later \n",
    "np.random.permutation(len(text_features))\n",
    "text_features, target = text_features[shuffle], target[shuffle]\n",
    "#separate into training and dev groups\n",
    "train_data, train_labels = text_features[:3200], target[:3200]\n",
    "dev_data, dev_labels = text_features[3200:], target[3200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.6842\n",
      "Accuracy of Bag-of-Words Random Forest Model: 0.6868\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.6842\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# get file names of all text files\n",
    "file_names = []\n",
    "for file in os.listdir(\"./case_txts\"):\n",
    "    if file.endswith(\"_clean.txt\"):\n",
    "        file_names.append(file)\n",
    "\n",
    "# open all texts and import into 1 list with each docket being 1 item in the list\n",
    "texts = []\n",
    "for f in file_names:\n",
    "    docket = open(\"./case_txts/\"+f, 'r')\n",
    "    texts.append(docket.readlines())\n",
    "    \n",
    "# get list of verdicts\n",
    "verdict = []\n",
    "verdict_csv = csv.reader(open('SCDB_2017_01_caseCentered_Citation.csv'))\n",
    "for row in verdict_csv:\n",
    "    verdict.append([row[13], row[36]])\n",
    "    \n",
    "# clean up file name to get docket number\n",
    "##### fix dockets with the word 'orig' in title\n",
    "docket_number = []\n",
    "for x in range(0, len(file_names)):\n",
    "    new_file_name = file_names[x].strip('_clean.txt')\n",
    "    new_file_name = new_file_name.strip('OA')\n",
    "    if new_file_name.count('_') >= 2:\n",
    "        new_file_name = new_file_name[:new_file_name.find('_', 4)]\n",
    "    new_file_name = new_file_name.replace('_', '-')\n",
    "    docket_number.append(new_file_name)\n",
    "\n",
    "# combine verdict (y variable) with the index of the texts (x variable)\n",
    "# 31 docket numbers that were not clean(includes random characters or can't find it at all) \n",
    "# so for the sake of simplicity for now I'm just excluding them from the training set\n",
    "winning_party = []\n",
    "texts_cleaned = []\n",
    "for x in range(0, len(docket_number)):\n",
    "    for y in range(0, len(verdict)):\n",
    "        if verdict[y][0] == docket_number[x]:\n",
    "            winning_party.append([docket_number[x], verdict[y][1]])\n",
    "            texts_cleaned.append(texts[x])\n",
    "\n",
    "# flatten texts so it can be used in CountVectorizer\n",
    "texts_cleaned = [item for sublist in texts_cleaned for item in sublist]\n",
    "texts_cleaned = np.array(texts_cleaned)\n",
    "\n",
    "# for reference:\n",
    "# verdict value 0 = no favorable disposition for petitioning part apparent\n",
    "# verdict value 1 = petitioning party received a favorable disposition\n",
    "# verdict value 2 = favorable disposition for petitioning party unclear\n",
    "# get only the verdict to use for y variable\n",
    "verdict_cleaned = []\n",
    "for y in range(0, len(winning_party)):\n",
    "    verdict_cleaned.append(int(winning_party[y][1]))\n",
    "verdict_cleaned = np.array(verdict_cleaned)\n",
    "\n",
    "#separate text and verdicts into train and test sets\n",
    "train_text = texts_cleaned[len(texts_cleaned)/3:]\n",
    "test_text = texts_cleaned[:len(texts_cleaned)/3]\n",
    "train_verdict = verdict_cleaned[len(texts_cleaned)/3:]\n",
    "test_verdict = verdict_cleaned[:len(verdict_cleaned)/3]\n",
    "\n",
    "# use CountVectorizer \n",
    "# could optimize by changing min_df only take counts above n\n",
    "count_vect = CountVectorizer(min_df = 3)\n",
    "x_train_counts = count_vect.fit_transform(train_text)\n",
    "\n",
    "# tf-idf to normalize word frequency\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
    "x_train_tfidf.toarray()\n",
    "\n",
    "# transform test data\n",
    "x_test_counts = count_vect.transform(test_text)\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "x_test_tfidf.toarray()\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "# could optimize by changing alpha\n",
    "clf = MultinomialNB(alpha = 0.1).fit(x_train_tfidf, train_verdict)\n",
    "naive_baynes_prediction = clf.predict(x_test_tfidf)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == test_verdict)\n",
    "print 'Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4)\n",
    "\n",
    "# Random Forest\n",
    "# could optimize by changing n_estimators\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train_tfidf, train_verdict)\n",
    "random_forest_prediction = forest.predict(x_test_tfidf)\n",
    "rf_accuracy = np.mean(random_forest_prediction == test_verdict)\n",
    "print 'Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4)\n",
    "\n",
    "# logistic regression\n",
    "# could optimize by changing C\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train_tfidf, train_verdict)\n",
    "lr_prediction = lr_model.predict(x_test_tfidf)\n",
    "lr_accuracy = np.mean(lr_prediction == test_verdict)\n",
    "print 'Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (760, 16181)\n",
      "x_test shape: (380, 16181)\n"
     ]
    }
   ],
   "source": [
    "print 'x_train shape:', x_train_tfidf.shape\n",
    "print 'x_test shape:', x_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
