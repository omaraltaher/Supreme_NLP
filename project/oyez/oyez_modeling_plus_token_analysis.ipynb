{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import column_or_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unzip and read turns files into pandas df\n",
    "zf1 = zipfile.ZipFile('./turns_part1.zip') \n",
    "turns1 = pd.read_csv(zf1.open('turns_part1.csv'))\n",
    "zf2 = zipfile.ZipFile('./turns_part2.zip') \n",
    "turns2 = pd.read_csv(zf2.open('turns_part2.csv'))\n",
    "zf3 = zipfile.ZipFile('./turns_part3.zip') \n",
    "turns3 = pd.read_csv(zf3.open('turns_part3.csv'))\n",
    "zf4 = zipfile.ZipFile('./turns_part4.zip') \n",
    "turns4 = pd.read_csv(zf4.open('turns_part4.csv'))\n",
    "\n",
    "#read summaries file into pandas df\n",
    "summaries = pd.read_csv('summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.8332\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.7822\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.8761\n",
      "Actual Count of y_test Verdicts:\n",
      " Petitioner    815\n",
      "Appellant     162\n",
      "Respondent     24\n",
      "Name: verdict, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#remove OA and _orig from transcript id in summaries df\n",
    "summaries['transcript_id'] = summaries['transcript_id'].str.replace('OA','')\n",
    "summaries['transcript_id'] = summaries['transcript_id'].str.replace('_orig','')\n",
    "\n",
    "#create new column for verdict variable\n",
    "def f(row):\n",
    "    if row['winning_party'] == row['first_party']:\n",
    "        val = row['first_party_label']\n",
    "    elif row['winning_party'] == row['second_party']:\n",
    "        val = row['second_party_label']\n",
    "    else:\n",
    "        val = 'No Verdict'\n",
    "    return val\n",
    "\n",
    "summaries['verdict'] = summaries.apply(f, axis=1)\n",
    "\n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(summaries.set_index('transcript_id')['verdict'], on='transcript_id')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df[train_test_df.verdict.notnull()]\n",
    "train_test_df = train_test_df.dropna()\n",
    "train_test_df = train_test_df[train_test_df['verdict'] != 'No Verdict']\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.verdict\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.851\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.7996\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.8839\n",
      "Actual Count of y_test Verdicts:\n",
      " Petitioner    803\n",
      "Appellant     155\n",
      "Respondent     14\n",
      "Appellee        1\n",
      "Name: verdict, dtype: int64\n",
      "LR predictions:\n",
      " (array(['Appellant', 'Petitioner', 'Respondent'], dtype=object), array([136, 835,   2]))\n"
     ]
    }
   ],
   "source": [
    "####rerunning above but using the case name instead of case id\n",
    "\n",
    "#create new column for verdict variable\n",
    "def f(row):\n",
    "    if row['winning_party'] == row['first_party']:\n",
    "        val = row['first_party_label']\n",
    "    elif row['winning_party'] == row['second_party']:\n",
    "        val = row['second_party_label']\n",
    "    else:\n",
    "        val = 'No Verdict'\n",
    "    return val\n",
    "\n",
    "summaries['verdict'] = summaries.apply(f, axis=1)\n",
    "\n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'title', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(summaries.set_index('case_name')['verdict'], on='title')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df[train_test_df.verdict.notnull()]\n",
    "train_test_df = train_test_df.dropna()\n",
    "train_test_df = train_test_df[train_test_df['verdict'] != 'No Verdict']\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.verdict\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())\n",
    "\n",
    "print('LR predictions:\\n', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.6169\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.5533\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.5715\n",
      "Actual Count of y_test Verdicts:\n",
      " 1    1120\n",
      "0     642\n",
      "Name: partyWinning, dtype: int64\n",
      "LR predictions:\n",
      " (array(['0', '1'], dtype=object), array([ 537, 1225]))\n"
     ]
    }
   ],
   "source": [
    "#use original scdb winning party as verdict\n",
    "#verdict value 0 = no favorable disposition for petitioning part apparent\n",
    "#verdict value 1 = petitioning party received a favorable disposition\n",
    "verdict = []\n",
    "verdict_csv = []\n",
    "\n",
    "with open('../SCDB_2017_01_caseCentered_Citation.csv', encoding=\"cp1252\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        verdict_csv.append(row)\n",
    "for row in verdict_csv:\n",
    "    docket_number = re.sub('-', '_', row[13])\n",
    "    docket_number = re.sub(' ORIG', '_orig', docket_number)\n",
    "    case_id = row[10]+'_'+docket_number\n",
    "    verdict.append([case_id, row[36]])\n",
    "    \n",
    "verdict_header = verdict.pop(0)\n",
    "verdict = pd.DataFrame(verdict, columns = verdict_header)\n",
    "    \n",
    "    \n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(verdict.set_index('term_docket')['partyWinning'], on='transcript_id')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df.dropna()\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.partyWinning\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())\n",
    "\n",
    "print('LR predictions:\\n', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexjamar/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:43: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "##SET UP\n",
    "\n",
    "#use original scdb winning party as verdict\n",
    "#verdict value 0 = no favorable disposition for petitioning part apparent\n",
    "#verdict value 1 = petitioning party received a favorable disposition\n",
    "verdict = []\n",
    "verdict_csv = []\n",
    "\n",
    "with open('../SCDB_2017_01_caseCentered_Citation.csv', encoding=\"cp1252\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        verdict_csv.append(row)\n",
    "        \n",
    "for row in verdict_csv:\n",
    "    docket_number = re.sub('-', '_', row[13])\n",
    "    docket_number = re.sub(' ORIG', '_orig', docket_number)\n",
    "    case_id = row[10]+'_'+docket_number\n",
    "    verdict.append([case_id, row[12], row[17], row[19], row[36], row[39], row[40]])\n",
    "    \n",
    "verdict_header = verdict.pop(0)\n",
    "verdict = pd.DataFrame(verdict, columns = verdict_header)\n",
    "    \n",
    "    \n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#get advocate sides\n",
    "advocates = pd.read_json('./advocate_dict.json')\n",
    "\n",
    "advocate_turns = []\n",
    "\n",
    "for index, row in turns_combined.iterrows():\n",
    "    if row['speaker_role'] == 'scotus_justice':\n",
    "        advocate_turns.append('scotus_justice')\n",
    "    else:\n",
    "        speaker = row['speaker']\n",
    "        transcript_id = row['transcript_id']\n",
    "        try:\n",
    "            lawyer_side = advocates.ix[speaker][transcript_id]\n",
    "            advocate_turns.append(lawyer_side)\n",
    "        except:\n",
    "            advocate_turns.append('None')\n",
    "\n",
    "#insert advocate side to the turns_combined dataframe\n",
    "turns_combined['lawyer_side'] = advocate_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PIVOT\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'lawyer_side', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#count of number of times speaker spoke\n",
    "counts_pivot = pd.pivot_table(turns_combined[['transcript_id', 'lawyer_side']],\n",
    "                              index = 'transcript_id',\n",
    "                              columns = 'lawyer_side',\n",
    "                              aggfunc=len,\n",
    "                              fill_value=0)\n",
    "\n",
    "#reset index\n",
    "counts_pivot = counts_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "counts_pivot = counts_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#rename headers\n",
    "counts_pivot.columns = ['transcript_id', 'appellant/petitioner_count', 'appellee/respondent_count',\n",
    "                       'scotus_justice_count']\n",
    "\n",
    "#length of speaker speaking\n",
    "length_pivot = pd.pivot_table(turns_combined[['transcript_id', 'text_duration', 'lawyer_side']],\n",
    "                              index = 'transcript_id',\n",
    "                              columns = 'lawyer_side',\n",
    "                              values = 'text_duration',\n",
    "                              aggfunc=np.sum,\n",
    "                              fill_value=0)\n",
    "\n",
    "#reset index\n",
    "length_pivot = length_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "length_pivot = length_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#rename headers\n",
    "length_pivot.columns = ['transcript_id', 'appellant/petitioner_length', 'appellee/respondent_length',\n",
    "                       'scotus_justice_length']\n",
    "\n",
    "#concatenate pivots together\n",
    "pivots_concate = pd.concat([speaker_role_text_pivot, \n",
    "                            counts_pivot[counts_pivot.columns[1:4]], \n",
    "                            length_pivot[length_pivot.columns[1:4]]],\n",
    "                            axis=1,\n",
    "                            join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_docket</th>\n",
       "      <th>chief</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>partyWinning</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946_24</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>80180</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946_12</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>10500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946_21</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>209</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>80250</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946_26</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>27</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>20150</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946_50</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>27</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>80060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term_docket   chief petitioner respondent partyWinning  issue issueArea\n",
       "0     1946_24  Vinson        198        172            1  80180         8\n",
       "1     1946_12  Vinson        100         27            0  10500         1\n",
       "2     1946_21  Vinson        209         27            0  80250         8\n",
       "3     1946_26  Vinson         27        170            0  20150         2\n",
       "4     1946_50  Vinson         27        176            1  80060         8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verdict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert attributes to ints\n",
    "verdict['petitioner'] = verdict['petitioner'].astype(int)\n",
    "verdict['respondent'] = verdict.respondent.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['respondent'] = verdict['respondent'].astype(int)\n",
    "verdict['issue'] = verdict.issue.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['issue'] = verdict['issue'].astype(int)\n",
    "verdict['issueArea'] = verdict.issueArea.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['issueArea'] = verdict['issueArea'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.6448\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.5705\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.576\n",
      "Actual Count of y_test Verdicts:\n",
      " (array(['0', '1'], dtype=object), array([323, 592]))\n",
      "RF predictions: (array(['0', '1'], dtype=object), array([ 32, 883]))\n",
      "NB predictions: (array(['0', '1'], \n",
      "      dtype='<U1'), array([374, 541]))\n",
      "LR predictions: (array(['0', '1'], dtype=object), array([333, 582]))\n"
     ]
    }
   ],
   "source": [
    "##MODELING\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = pivots_concate.join(verdict.set_index('term_docket'), on='transcript_id')\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df.dropna()\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['appellant/petitioner', 'appellee/respondent', 'scotus_justice', 'chief',\n",
    "                   'appellant/petitioner_count', 'appellee/respondent_count', 'scotus_justice_count',\n",
    "                   'appellant/petitioner_length', 'appellee/respondent_length','scotus_justice_length',\n",
    "                   'petitioner', 'respondent', 'issue', 'issueArea']]\n",
    "y = train_test_df.partyWinning\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect_petitioner = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_respondent = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_justice = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_chief = CountVectorizer()\n",
    "\n",
    "x_train_cv_petitionervec = count_vect_petitioner.fit(x_train['appellant/petitioner'])\n",
    "x_train_cv_respondentvec = count_vect_respondent.fit(x_train['appellee/respondent'])\n",
    "x_train_cv_scotus_justicevec = count_vect_justice.fit(x_train['scotus_justice'])\n",
    "x_train_chief = count_vect_chief.fit(x_train['chief'])\n",
    "\n",
    "x_test_cv_petitionervec = count_vect_petitioner.fit(x_test['appellant/petitioner'])\n",
    "x_test_cv_respondentvec = count_vect_respondent.fit(x_test['appellee/respondent'])\n",
    "x_test_cv_scotus_justicevec = count_vect_justice.fit(x_test['scotus_justice'])\n",
    "x_test_chief = count_vect_chief.fit(x_test['chief'])\n",
    "\n",
    "x_train_cv_petitioner = pd.DataFrame(x_train_cv_petitionervec.transform(x_train['appellant/petitioner']).todense(),\n",
    "                                        columns = x_train_cv_petitionervec.get_feature_names())\n",
    "x_train_cv_respondant = pd.DataFrame(x_train_cv_respondantvec.transform(x_train['appellee/respondent']).todense(),\n",
    "                                        columns = x_train_cv_respondantvec.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justicevec.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justicevec.get_feature_names())\n",
    "x_train_cv_chief = pd.DataFrame(x_train_chief.transform(x_train['chief']).todense(),\n",
    "                               columns = x_train_chief.get_feature_names())\n",
    "\n",
    "x_test_cv_petitioner = pd.DataFrame(x_test_cv_petitionervec.transform(x_test['appellant/petitioner']).todense(),\n",
    "                                        columns = x_test_cv_petitionervec.get_feature_names())\n",
    "x_test_cv_respondant = pd.DataFrame(x_test_cv_respondantvec.transform(x_test['appellee/respondent']).todense(),\n",
    "                                        columns = x_test_cv_respondantvec.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justicevec.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justicevec.get_feature_names())\n",
    "x_test_cv_chief = pd.DataFrame(x_test_chief.transform(x_test['chief']).todense(),\n",
    "                               columns = x_test_chief.get_feature_names())\n",
    "\n",
    "#put features into its own df for concatenate in next step\n",
    "x_train_features = x_train[x_train.columns[4:]]\n",
    "x_train_features = x_train_features.reset_index()\n",
    "x_test_features = x_test[x_train.columns[4:]]\n",
    "x_test_features = x_test_features.reset_index()\n",
    "\n",
    "#rest y train + test indices, drop 'index' columns, and convert to 1-d matrices\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.reset_index()\n",
    "y_train = y_train.drop(['index'], axis = 1)\n",
    "y_test = y_test.drop(['index'], axis = 1)\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "y_train = column_or_1d(y_train)\n",
    "y_test = column_or_1d(y_test)\n",
    "\n",
    "\n",
    "#change negative numbers into 0\n",
    "x_train_features[x_train_features < 0 ] = 0\n",
    "x_test_features[x_test_features < 0 ] = 0\n",
    "\n",
    "#concatenate the bow back\n",
    "x_train = pd.concat([x_train_cv_petitioner, \n",
    "                     x_train_cv_respondant, \n",
    "                     x_train_cv_scotus_justice,\n",
    "                     x_train_cv_chief,\n",
    "                     x_train_features], \n",
    "                     axis = 1)\n",
    "x_test = pd.concat([x_test_cv_petitioner, \n",
    "                    x_test_cv_respondant, \n",
    "                    x_test_cv_scotus_justice,\n",
    "                    x_test_cv_chief,\n",
    "                    x_test_features],\n",
    "                    axis = 1)\n",
    "\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l1\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', np.unique(y_test, return_counts=True))\n",
    "\n",
    "print('RF predictions:', np.unique(random_forest_prediction, return_counts=True))\n",
    "print('NB predictions:', np.unique(naive_baynes_prediction, return_counts=True))\n",
    "print('LR predictions:', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = [\"partyWinning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>about it</th>\n",
       "      <th>about that</th>\n",
       "      <th>about the</th>\n",
       "      <th>about this</th>\n",
       "      <th>about what</th>\n",
       "      <th>about whether</th>\n",
       "      <th>absence of</th>\n",
       "      <th>access to</th>\n",
       "      <th>...</th>\n",
       "      <th>appellee/respondent_count</th>\n",
       "      <th>scotus_justice_count</th>\n",
       "      <th>appellant/petitioner_length</th>\n",
       "      <th>appellee/respondent_length</th>\n",
       "      <th>scotus_justice_length</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>partyWinning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>113</td>\n",
       "      <td>1417.802</td>\n",
       "      <td>1268.599</td>\n",
       "      <td>882.820</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>30010.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>1271.119</td>\n",
       "      <td>1410.646</td>\n",
       "      <td>792.277</td>\n",
       "      <td>208.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>90090.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>151.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>70050.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>107</td>\n",
       "      <td>1157.113</td>\n",
       "      <td>1237.199</td>\n",
       "      <td>1242.412</td>\n",
       "      <td>137.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10390.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>139</td>\n",
       "      <td>1030.501</td>\n",
       "      <td>988.384</td>\n",
       "      <td>1644.819</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>90150.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability to  able to  about it  about that  about the  about this  \\\n",
       "0           0        0         0           0          3           0   \n",
       "1           0        0         0           0          1           0   \n",
       "2           0        1         1           1          2           1   \n",
       "3           0        1         0           0          1           0   \n",
       "4           0        0         0           0          0           1   \n",
       "\n",
       "   about what  about whether  absence of  access to      ...       \\\n",
       "0           0              0           0          5      ...        \n",
       "1           0              0           1          0      ...        \n",
       "2           0              0           0          0      ...        \n",
       "3           0              0           0          0      ...        \n",
       "4           1              0           1          1      ...        \n",
       "\n",
       "   appellee/respondent_count  scotus_justice_count  \\\n",
       "0                         77                   113   \n",
       "1                         40                    73   \n",
       "2                         93                     4   \n",
       "3                         57                   107   \n",
       "4                         68                   139   \n",
       "\n",
       "   appellant/petitioner_length  appellee/respondent_length  \\\n",
       "0                     1417.802                    1268.599   \n",
       "1                     1271.119                    1410.646   \n",
       "2                        0.000                       0.000   \n",
       "3                     1157.113                    1237.199   \n",
       "4                     1030.501                     988.384   \n",
       "\n",
       "   scotus_justice_length  petitioner  respondent    issue  issueArea  \\\n",
       "0                882.820       249.0       249.0  30010.0        3.0   \n",
       "1                792.277       208.0       143.0  90090.0        9.0   \n",
       "2                  0.000       151.0       371.0  70050.0        7.0   \n",
       "3               1242.412       137.0        27.0  10390.0        1.0   \n",
       "4               1644.819       100.0        28.0  90150.0        9.0   \n",
       "\n",
       "   partyWinning  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 29615 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "petitioner_wins = train_concat[train_concat['partyWinning'] == '1']\n",
    "petitioner_wins = petitioner_wins.append(petitioner_wins.apply(np.count_nonzero, axis = 0), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>about it</th>\n",
       "      <th>about that</th>\n",
       "      <th>about the</th>\n",
       "      <th>about this</th>\n",
       "      <th>about what</th>\n",
       "      <th>about whether</th>\n",
       "      <th>absence of</th>\n",
       "      <th>access to</th>\n",
       "      <th>...</th>\n",
       "      <th>appellee/respondent_count</th>\n",
       "      <th>scotus_justice_count</th>\n",
       "      <th>appellant/petitioner_length</th>\n",
       "      <th>appellee/respondent_length</th>\n",
       "      <th>scotus_justice_length</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>partyWinning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30010.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>101</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>102.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>70180.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>140</td>\n",
       "      <td>1116.45</td>\n",
       "      <td>1041.375</td>\n",
       "      <td>1329.769</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>40010.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>186.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>211</td>\n",
       "      <td>528</td>\n",
       "      <td>234</td>\n",
       "      <td>304</td>\n",
       "      <td>726</td>\n",
       "      <td>181</td>\n",
       "      <td>202</td>\n",
       "      <td>190</td>\n",
       "      <td>198</td>\n",
       "      <td>137</td>\n",
       "      <td>...</td>\n",
       "      <td>1158</td>\n",
       "      <td>1158</td>\n",
       "      <td>699.00</td>\n",
       "      <td>698.000</td>\n",
       "      <td>668.000</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability to  able to  about it  about that  about the  about this  \\\n",
       "1154           0        0         0           0          2           0   \n",
       "1155           0        0         1           0          1           0   \n",
       "1156           1        2         0           0          0           0   \n",
       "1157           0        2         0           0          1           0   \n",
       "1158         211      528       234         304        726         181   \n",
       "\n",
       "      about what  about whether  absence of  access to      ...       \\\n",
       "1154           0              0           0          0      ...        \n",
       "1155           0              0           0          0      ...        \n",
       "1156           0              0           0          0      ...        \n",
       "1157           0              0           0          0      ...        \n",
       "1158         202            190         198        137      ...        \n",
       "\n",
       "      appellee/respondent_count  scotus_justice_count  \\\n",
       "1154                         42                    70   \n",
       "1155                         78                   101   \n",
       "1156                         74                   140   \n",
       "1157                          1                    12   \n",
       "1158                       1158                  1158   \n",
       "\n",
       "      appellant/petitioner_length  appellee/respondent_length  \\\n",
       "1154                         0.00                       0.000   \n",
       "1155                         0.00                       0.000   \n",
       "1156                      1116.45                    1041.375   \n",
       "1157                         0.00                       0.000   \n",
       "1158                       699.00                     698.000   \n",
       "\n",
       "      scotus_justice_length  petitioner  respondent    issue  issueArea  \\\n",
       "1154                  0.000         8.0        21.0  30010.0        3.0   \n",
       "1155                  0.000       102.0       146.0  70180.0        7.0   \n",
       "1156               1329.769       100.0        28.0  40010.0        4.0   \n",
       "1157                  0.000       186.0       324.0  10020.0        1.0   \n",
       "1158                668.000      1158.0      1158.0   1158.0     1158.0   \n",
       "\n",
       "      partyWinning  \n",
       "1154             1  \n",
       "1155             1  \n",
       "1156             1  \n",
       "1157             1  \n",
       "1158          1158  \n",
       "\n",
       "[5 rows x 29615 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitioner_wins.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "respondent_wins = train_concat[train_concat['partyWinning'] == '0']\n",
    "respondent_wins = respondent_wins.append(respondent_wins.apply(np.count_nonzero, axis = 0), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>about it</th>\n",
       "      <th>about that</th>\n",
       "      <th>about the</th>\n",
       "      <th>about this</th>\n",
       "      <th>about what</th>\n",
       "      <th>about whether</th>\n",
       "      <th>absence of</th>\n",
       "      <th>access to</th>\n",
       "      <th>...</th>\n",
       "      <th>appellee/respondent_count</th>\n",
       "      <th>scotus_justice_count</th>\n",
       "      <th>appellant/petitioner_length</th>\n",
       "      <th>appellee/respondent_length</th>\n",
       "      <th>scotus_justice_length</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>partyWinning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>116</td>\n",
       "      <td>1470.044</td>\n",
       "      <td>1322.339</td>\n",
       "      <td>868.866</td>\n",
       "      <td>4.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>80100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>20210.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>222</td>\n",
       "      <td>1443.440</td>\n",
       "      <td>1251.734</td>\n",
       "      <td>2485.569</td>\n",
       "      <td>126.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>100020.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>194</td>\n",
       "      <td>1323.691</td>\n",
       "      <td>926.045</td>\n",
       "      <td>794.287</td>\n",
       "      <td>126.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>140</td>\n",
       "      <td>343</td>\n",
       "      <td>149</td>\n",
       "      <td>179</td>\n",
       "      <td>441</td>\n",
       "      <td>105</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>92</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>697</td>\n",
       "      <td>697</td>\n",
       "      <td>422.000</td>\n",
       "      <td>427.000</td>\n",
       "      <td>411.000</td>\n",
       "      <td>697.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ability to  able to  about it  about that  about the  about this  \\\n",
       "693           0        1         1           1          2           0   \n",
       "694           0        1         0           0          2           0   \n",
       "695           1        0         0           0          2           0   \n",
       "696           2        8         0           0          0           0   \n",
       "697         140      343       149         179        441         105   \n",
       "\n",
       "     about what  about whether  absence of  access to      ...       \\\n",
       "693           0              0           0          0      ...        \n",
       "694           0              0           0          0      ...        \n",
       "695           2              1           1          0      ...        \n",
       "696           0              0           0          0      ...        \n",
       "697         106            106          92         76      ...        \n",
       "\n",
       "     appellee/respondent_count  scotus_justice_count  \\\n",
       "693                         39                   116   \n",
       "694                         78                   150   \n",
       "695                        101                   222   \n",
       "696                         68                   194   \n",
       "697                        697                   697   \n",
       "\n",
       "     appellant/petitioner_length  appellee/respondent_length  \\\n",
       "693                     1470.044                    1322.339   \n",
       "694                        0.000                       0.000   \n",
       "695                     1443.440                    1251.734   \n",
       "696                     1323.691                     926.045   \n",
       "697                      422.000                     427.000   \n",
       "\n",
       "     scotus_justice_length  petitioner  respondent     issue  issueArea  \\\n",
       "693                868.866         4.0       198.0   80100.0        8.0   \n",
       "694                  0.000        21.0       239.0   20210.0        2.0   \n",
       "695               2485.569       126.0        28.0  100020.0       10.0   \n",
       "696                794.287       126.0        27.0   30200.0        3.0   \n",
       "697                411.000       697.0       697.0     689.0      689.0   \n",
       "\n",
       "     partyWinning  \n",
       "693             0  \n",
       "694             0  \n",
       "695             0  \n",
       "696             0  \n",
       "697           697  \n",
       "\n",
       "[5 rows x 29615 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respondent_wins.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your your\n",
      "your your\n"
     ]
    }
   ],
   "source": [
    "#Index of the last token token\n",
    "print(petitioner_wins.columns[29598])\n",
    "print(respondent_wins.columns[29598])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-BOW Columns\n",
    "petitioner_wins = petitioner_wins.loc[:, :\"your your\"]\n",
    "respondent_wins = respondent_wins.loc[:, :\"your your\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens when petitioner wins: \n",
      "in the       1158\n",
      "the court    1158\n",
      "in           1158\n",
      "it           1158\n",
      "of           1158\n",
      "that         1158\n",
      "the          1158\n",
      "to           1158\n",
      "of the       1157\n",
      "that the     1157\n",
      "Name: 1158, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Most common tokens when respondent wins: \n",
      "in the    697\n",
      "of the    697\n",
      "and       697\n",
      "as        697\n",
      "be        697\n",
      "for       697\n",
      "have      697\n",
      "in        697\n",
      "is        697\n",
      "it        697\n",
      "Name: 697, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Find 10 most common tokens when petitioner/respondent wins\n",
    "petitioner_wins_most_common = petitioner_wins.iloc[-1].nlargest(10)\n",
    "respondent_wins_most_common = respondent_wins.iloc[-1].nlargest(10)\n",
    "print(\"Most common tokens when petitioner wins: \\n\", petitioner_wins_most_common, sep = '')\n",
    "print(\"-\" * 100)\n",
    "print(\"Most common tokens when respondent wins: \\n\", respondent_wins_most_common, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the tokens never used when the petitioner/respondent wins\n",
    "zero_counts_petitioner_wins = petitioner_wins.columns[petitioner_wins.iloc[-1] == 0]\n",
    "zero_counts_respondent_wins = respondent_wins.columns[respondent_wins.iloc[-1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find tokens used when petitioner wins but not when respondent wins and vice versa\n",
    "token_only_present_when_respondent_wins = {}\n",
    "token_only_present_when_petitioner_wins = {}\n",
    "for token in list(zero_counts_petitioner_wins):\n",
    "    if token not in list(zero_counts_respondent_wins):\n",
    "        token_only_present_when_respondent_wins[token] = respondent_wins.iloc[-1][token]\n",
    "for t in list(zero_counts_respondent_wins):\n",
    "    if t not in list(zero_counts_petitioner_wins):\n",
    "        token_only_present_when_petitioner_wins[t] = petitioner_wins.iloc[-1][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to DF\n",
    "token_only_present_when_respondent_wins = pd.DataFrame(token_only_present_when_respondent_wins, \n",
    "                                                       index = [0])\n",
    "token_only_present_when_petitioner_wins = pd.DataFrame(token_only_present_when_petitioner_wins,\n",
    "                                                       index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens only present when petitioner wins: \n",
      "watershed        9\n",
      "apportioning     6\n",
      "disparagement    5\n",
      "draftsmanship    5\n",
      "elevation        5\n",
      "explosive        5\n",
      "invalidates      5\n",
      "lessees          5\n",
      "multistate       5\n",
      "oust             5\n",
      "Name: 0, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Most common tokens only present when respondent wins: \n",
      "personality    15\n",
      "schneckloth    13\n",
      "cat            12\n",
      "disappears     11\n",
      "theater        11\n",
      "thou           11\n",
      "tight          11\n",
      "litigations    10\n",
      "servant        10\n",
      "112             9\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Find the most common tokens used when the respondent wins but not present when the petitioner wins and vice versa\n",
    "most_common_token_only_present_when_respondent_wins = token_only_present_when_respondent_wins.iloc[0].nlargest(10)\n",
    "most_common_token_only_present_when_petitioner_wins = token_only_present_when_petitioner_wins.iloc[0].nlargest(10)\n",
    "print(\"Most common tokens only present when petitioner wins: \\n\", most_common_token_only_present_when_respondent_wins,\n",
    "      sep = '')\n",
    "print(\"-\" * 100)\n",
    "print(\"Most common tokens only present when respondent wins: \\n\", most_common_token_only_present_when_petitioner_wins,\n",
    "      sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
