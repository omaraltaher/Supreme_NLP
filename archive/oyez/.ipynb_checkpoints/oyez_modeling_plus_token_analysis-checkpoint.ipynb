{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import column_or_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unzip and read turns files into pandas df\n",
    "zf1 = zipfile.ZipFile('./turns_part1.zip') \n",
    "turns1 = pd.read_csv(zf1.open('turns_part1.csv'))\n",
    "zf2 = zipfile.ZipFile('./turns_part2.zip') \n",
    "turns2 = pd.read_csv(zf2.open('turns_part2.csv'))\n",
    "zf3 = zipfile.ZipFile('./turns_part3.zip') \n",
    "turns3 = pd.read_csv(zf3.open('turns_part3.csv'))\n",
    "zf4 = zipfile.ZipFile('./turns_part4.zip') \n",
    "turns4 = pd.read_csv(zf4.open('turns_part4.csv'))\n",
    "\n",
    "#read summaries file into pandas df\n",
    "summaries = pd.read_csv('summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of Bag-of-Words Random Forest Model:', 0.8282)\n",
      "('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', 0.8322)\n",
      "('Accuracy of Bag-of-Words Logistic Regression Model:', 0.8831)\n",
      "('Actual Count of y_test Verdicts:\\n', Petitioner    809\n",
      "Appellant     173\n",
      "Respondent     19\n",
      "Name: verdict, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#remove OA and _orig from transcript id in summaries df\n",
    "summaries['transcript_id'] = summaries['transcript_id'].str.replace('OA','')\n",
    "summaries['transcript_id'] = summaries['transcript_id'].str.replace('_orig','')\n",
    "\n",
    "#create new column for verdict variable\n",
    "def f(row):\n",
    "    if row['winning_party'] == row['first_party']:\n",
    "        val = row['first_party_label']\n",
    "    elif row['winning_party'] == row['second_party']:\n",
    "        val = row['second_party_label']\n",
    "    else:\n",
    "        val = 'No Verdict'\n",
    "    return val\n",
    "\n",
    "summaries['verdict'] = summaries.apply(f, axis=1)\n",
    "\n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(summaries.set_index('transcript_id')['verdict'], on='transcript_id')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df[train_test_df.verdict.notnull()]\n",
    "train_test_df = train_test_df.dropna()\n",
    "train_test_df = train_test_df[train_test_df['verdict'] != 'No Verdict']\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.verdict\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of Bag-of-Words Random Forest Model:', 0.853)\n",
      "('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', 0.8201)\n",
      "('Accuracy of Bag-of-Words Logistic Regression Model:', 0.889)\n",
      "('Actual Count of y_test Verdicts:\\n', Petitioner    808\n",
      "Appellant     148\n",
      "Respondent     16\n",
      "Appellee        1\n",
      "Name: verdict, dtype: int64)\n",
      "('LR predictions:\\n', (array(['Appellant', 'Petitioner', 'Respondent'], dtype=object), array([140, 830,   3])))\n"
     ]
    }
   ],
   "source": [
    "####rerunning above but using the case name instead of case id\n",
    "\n",
    "#create new column for verdict variable\n",
    "def f(row):\n",
    "    if row['winning_party'] == row['first_party']:\n",
    "        val = row['first_party_label']\n",
    "    elif row['winning_party'] == row['second_party']:\n",
    "        val = row['second_party_label']\n",
    "    else:\n",
    "        val = 'No Verdict'\n",
    "    return val\n",
    "\n",
    "summaries['verdict'] = summaries.apply(f, axis=1)\n",
    "\n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'title', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(summaries.set_index('case_name')['verdict'], on='title')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df[train_test_df.verdict.notnull()]\n",
    "train_test_df = train_test_df.dropna()\n",
    "train_test_df = train_test_df[train_test_df['verdict'] != 'No Verdict']\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.verdict\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())\n",
    "\n",
    "print('LR predictions:\\n', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of Bag-of-Words Random Forest Model:', 0.6379)\n",
      "('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', 0.5755)\n",
      "('Accuracy of Bag-of-Words Logistic Regression Model:', 0.5522)\n",
      "('Actual Count of y_test Verdicts:\\n', 1    1132\n",
      "0     630\n",
      "Name: partyWinning, dtype: int64)\n",
      "('LR predictions:\\n', (array(['0', '1'], dtype=object), array([ 589, 1173])))\n"
     ]
    }
   ],
   "source": [
    "#use original scdb winning party as verdict\n",
    "#verdict value 0 = no favorable disposition for petitioning part apparent\n",
    "#verdict value 1 = petitioning party received a favorable disposition\n",
    "verdict = []\n",
    "verdict_csv = []\n",
    "\n",
    "with open('../SCDB_2017_01_caseCentered_Citation.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        verdict_csv.append(row)\n",
    "for row in verdict_csv:\n",
    "    docket_number = re.sub('-', '_', row[13])\n",
    "    docket_number = re.sub(' ORIG', '_orig', docket_number)\n",
    "    case_id = row[10]+'_'+docket_number\n",
    "    verdict.append([case_id, row[36]])\n",
    "    \n",
    "verdict_header = verdict.pop(0)\n",
    "verdict = pd.DataFrame(verdict, columns = verdict_header)\n",
    "    \n",
    "    \n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(verdict.set_index('term_docket')['partyWinning'], on='transcript_id')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df.dropna()\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.partyWinning\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', y_test.value_counts())\n",
    "\n",
    "print('LR predictions:\\n', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pzhou11/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:43: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    }
   ],
   "source": [
    "##SET UP\n",
    "\n",
    "#use original scdb winning party as verdict\n",
    "#verdict value 0 = no favorable disposition for petitioning part apparent\n",
    "#verdict value 1 = petitioning party received a favorable disposition\n",
    "verdict = []\n",
    "verdict_csv = []\n",
    "\n",
    "with open('../SCDB_2017_01_caseCentered_Citation.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        verdict_csv.append(row)\n",
    "        \n",
    "for row in verdict_csv:\n",
    "    docket_number = re.sub('-', '_', row[13])\n",
    "    docket_number = re.sub(' ORIG', '_orig', docket_number)\n",
    "    case_id = row[10]+'_'+docket_number\n",
    "    verdict.append([case_id, row[12], row[17], row[19], row[36], row[39], row[40]])\n",
    "    \n",
    "verdict_header = verdict.pop(0)\n",
    "verdict = pd.DataFrame(verdict, columns = verdict_header)\n",
    "    \n",
    "    \n",
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "#get advocate sides\n",
    "advocates = pd.read_json('./advocate_dict.json')\n",
    "\n",
    "advocate_turns = []\n",
    "\n",
    "for index, row in turns_combined.iterrows():\n",
    "    if row['speaker_role'] == 'scotus_justice':\n",
    "        advocate_turns.append('scotus_justice')\n",
    "    else:\n",
    "        speaker = row['speaker']\n",
    "        transcript_id = row['transcript_id']\n",
    "        try:\n",
    "            lawyer_side = advocates.ix[speaker][transcript_id]\n",
    "            advocate_turns.append(lawyer_side)\n",
    "        except:\n",
    "            advocate_turns.append('None')\n",
    "\n",
    "#insert advocate side to the turns_combined dataframe\n",
    "turns_combined['lawyer_side'] = advocate_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##PIVOT\n",
    "\n",
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'lawyer_side', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#count of number of times speaker spoke\n",
    "counts_pivot = pd.pivot_table(turns_combined[['transcript_id', 'lawyer_side']],\n",
    "                              index = 'transcript_id',\n",
    "                              columns = 'lawyer_side',\n",
    "                              aggfunc=len,\n",
    "                              fill_value=0)\n",
    "\n",
    "#reset index\n",
    "counts_pivot = counts_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "counts_pivot = counts_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#rename headers\n",
    "counts_pivot.columns = ['transcript_id', 'appellant/petitioner_count', 'appellee/respondent_count',\n",
    "                       'scotus_justice_count']\n",
    "\n",
    "#length of speaker speaking\n",
    "length_pivot = pd.pivot_table(turns_combined[['transcript_id', 'text_duration', 'lawyer_side']],\n",
    "                              index = 'transcript_id',\n",
    "                              columns = 'lawyer_side',\n",
    "                              values = 'text_duration',\n",
    "                              aggfunc=np.sum,\n",
    "                              fill_value=0)\n",
    "\n",
    "#reset index\n",
    "length_pivot = length_pivot.reset_index()\n",
    "\n",
    "#drop columns with no lawyer side tags\n",
    "length_pivot = length_pivot.drop(['NEED MORE INFO', 'None'], axis = 1)\n",
    "\n",
    "#rename headers\n",
    "length_pivot.columns = ['transcript_id', 'appellant/petitioner_length', 'appellee/respondent_length',\n",
    "                       'scotus_justice_length']\n",
    "\n",
    "#concatenate pivots together\n",
    "pivots_concate = pd.concat([speaker_role_text_pivot, \n",
    "                            counts_pivot[counts_pivot.columns[1:4]], \n",
    "                            length_pivot[length_pivot.columns[1:4]]],\n",
    "                            axis=1,\n",
    "                            join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_docket</th>\n",
       "      <th>chief</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>partyWinning</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946_24</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>80180</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946_12</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>10500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946_21</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>209</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>80250</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946_26</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>27</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>20150</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946_50</td>\n",
       "      <td>Vinson</td>\n",
       "      <td>27</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>80060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term_docket   chief petitioner respondent partyWinning  issue issueArea\n",
       "0     1946_24  Vinson        198        172            1  80180         8\n",
       "1     1946_12  Vinson        100         27            0  10500         1\n",
       "2     1946_21  Vinson        209         27            0  80250         8\n",
       "3     1946_26  Vinson         27        170            0  20150         2\n",
       "4     1946_50  Vinson         27        176            1  80060         8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verdict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert attributes to ints\n",
    "verdict['petitioner'] = verdict['petitioner'].astype(int)\n",
    "verdict['respondent'] = verdict.respondent.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['respondent'] = verdict['respondent'].astype(int)\n",
    "verdict['issue'] = verdict.issue.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['issue'] = verdict['issue'].astype(int)\n",
    "verdict['issueArea'] = verdict.issueArea.apply(lambda x: 0 if x == '' else x)\n",
    "verdict['issueArea'] = verdict['issueArea'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of Bag-of-Words Random Forest Model:', 0.6404)\n",
      "('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', 0.5148)\n",
      "('Accuracy of Bag-of-Words Logistic Regression Model:', 0.6077)\n",
      "('Actual Count of y_test Verdicts:\\n', (array(['0', '1'], dtype=object), array([330, 585])))\n",
      "('RF predictions:', (array(['0', '1'], dtype=object), array([ 35, 880])))\n",
      "('NB predictions:', (array(['0', '1'], \n",
      "      dtype='|S1'), array([426, 489])))\n",
      "('LR predictions:', (array(['0', '1'], dtype=object), array([361, 554])))\n"
     ]
    }
   ],
   "source": [
    "##MODELING\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = pivots_concate.join(verdict.set_index('term_docket'), on='transcript_id')\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df.dropna()\n",
    "\n",
    "#create train and test split\n",
    "x = train_test_df[['appellant/petitioner', 'appellee/respondent', 'scotus_justice', 'chief',\n",
    "                   'appellant/petitioner_count', 'appellee/respondent_count', 'scotus_justice_count',\n",
    "                   'appellant/petitioner_length', 'appellee/respondent_length','scotus_justice_length',\n",
    "                   'petitioner', 'respondent', 'issue', 'issueArea']]\n",
    "y = train_test_df.partyWinning\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "#count vectorizer\n",
    "count_vect_petitioner = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_respondent = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_justice = CountVectorizer(ngram_range = (2,3), max_features = 2500, analyzer = 'word')\n",
    "count_vect_chief = CountVectorizer()\n",
    "\n",
    "x_train_cv_petitionervec = count_vect_petitioner.fit(x_train['appellant/petitioner'])\n",
    "x_train_cv_respondentvec = count_vect_respondent.fit(x_train['appellee/respondent'])\n",
    "x_train_cv_scotus_justicevec = count_vect_justice.fit(x_train['scotus_justice'])\n",
    "x_train_chief = count_vect_chief.fit(x_train['chief'])\n",
    "\n",
    "x_test_cv_petitionervec = count_vect_petitioner.fit(x_test['appellant/petitioner'])\n",
    "x_test_cv_respondentvec = count_vect_respondent.fit(x_test['appellee/respondent'])\n",
    "x_test_cv_scotus_justicevec = count_vect_justice.fit(x_test['scotus_justice'])\n",
    "x_test_chief = count_vect_chief.fit(x_test['chief'])\n",
    "\n",
    "x_train_cv_petitioner = pd.DataFrame(x_train_cv_petitionervec.transform(x_train['appellant/petitioner']).todense(),\n",
    "                                        columns = x_train_cv_petitionervec.get_feature_names())\n",
    "x_train_cv_respondent = pd.DataFrame(x_train_cv_respondentvec.transform(x_train['appellee/respondent']).todense(),\n",
    "                                        columns = x_train_cv_respondentvec.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justicevec.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justicevec.get_feature_names())\n",
    "x_train_cv_chief = pd.DataFrame(x_train_chief.transform(x_train['chief']).todense(),\n",
    "                               columns = x_train_chief.get_feature_names())\n",
    "\n",
    "x_test_cv_petitioner = pd.DataFrame(x_test_cv_petitionervec.transform(x_test['appellant/petitioner']).todense(),\n",
    "                                        columns = x_test_cv_petitionervec.get_feature_names())\n",
    "x_test_cv_respondent = pd.DataFrame(x_test_cv_respondentvec.transform(x_test['appellee/respondent']).todense(),\n",
    "                                        columns = x_test_cv_respondentvec.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justicevec.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justicevec.get_feature_names())\n",
    "x_test_cv_chief = pd.DataFrame(x_test_chief.transform(x_test['chief']).todense(),\n",
    "                               columns = x_test_chief.get_feature_names())\n",
    "\n",
    "#put features into its own df for concatenate in next step\n",
    "x_train_features = x_train[x_train.columns[4:]]\n",
    "x_train_features = x_train_features.reset_index()\n",
    "x_test_features = x_test[x_train.columns[4:]]\n",
    "x_test_features = x_test_features.reset_index()\n",
    "\n",
    "#rest y train + test indices, drop 'index' columns, and convert to 1-d matrices\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.reset_index()\n",
    "y_train = y_train.drop(['index'], axis = 1)\n",
    "y_test = y_test.drop(['index'], axis = 1)\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "y_train = column_or_1d(y_train)\n",
    "y_test = column_or_1d(y_test)\n",
    "\n",
    "\n",
    "#change negative numbers into 0\n",
    "x_train_features[x_train_features < 0 ] = 0\n",
    "x_test_features[x_test_features < 0 ] = 0\n",
    "\n",
    "#concatenate the bow back\n",
    "x_train = pd.concat([x_train_cv_petitioner, \n",
    "                     x_train_cv_respondent, \n",
    "                     x_train_cv_scotus_justice,\n",
    "                     x_train_cv_chief,\n",
    "                     x_train_features], \n",
    "                     axis = 1)\n",
    "x_test = pd.concat([x_test_cv_petitioner, \n",
    "                    x_test_cv_respondent, \n",
    "                    x_test_cv_scotus_justice,\n",
    "                    x_test_cv_chief,\n",
    "                    x_test_features],\n",
    "                    axis = 1)\n",
    "\n",
    "\n",
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l1\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))\n",
    "\n",
    "print('Actual Count of y_test Verdicts:\\n', np.unique(y_test, return_counts=True))\n",
    "\n",
    "print('RF predictions:', np.unique(random_forest_prediction, return_counts=True))\n",
    "print('NB predictions:', np.unique(naive_baynes_prediction, return_counts=True))\n",
    "print('LR predictions:', np.unique(lr_prediction, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = [\"partyWinning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_concat = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "petitioner_wins = train_concat[train_concat['partyWinning'] == '1']\n",
    "petitioner_wins = petitioner_wins.append(petitioner_wins.apply(np.count_nonzero, axis = 0), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petitioner_wins.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "respondent_wins = train_concat[train_concat['partyWinning'] == '0']\n",
    "respondent_wins = respondent_wins.append(respondent_wins.apply(np.count_nonzero, axis = 0), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondent_wins.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of the last token token\n",
    "print(petitioner_wins.columns[29598])\n",
    "print(respondent_wins.columns[29598])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove non-BOW Columns\n",
    "petitioner_wins = petitioner_wins.loc[:, :\"your your\"]\n",
    "respondent_wins = respondent_wins.loc[:, :\"your your\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find 10 most common tokens when petitioner/respondent wins\n",
    "petitioner_wins_most_common = petitioner_wins.iloc[-1].nlargest(10)\n",
    "respondent_wins_most_common = respondent_wins.iloc[-1].nlargest(10)\n",
    "print(\"Most common tokens when petitioner wins: \\n\", petitioner_wins_most_common, sep = '')\n",
    "print(\"-\" * 100)\n",
    "print(\"Most common tokens when respondent wins: \\n\", respondent_wins_most_common, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the tokens never used when the petitioner/respondent wins\n",
    "zero_counts_petitioner_wins = petitioner_wins.columns[petitioner_wins.iloc[-1] == 0]\n",
    "zero_counts_respondent_wins = respondent_wins.columns[respondent_wins.iloc[-1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find tokens used when petitioner wins but not when respondent wins and vice versa\n",
    "token_only_present_when_respondent_wins = {}\n",
    "token_only_present_when_petitioner_wins = {}\n",
    "for token in list(zero_counts_petitioner_wins):\n",
    "    if token not in list(zero_counts_respondent_wins):\n",
    "        token_only_present_when_respondent_wins[token] = respondent_wins.iloc[-1][token]\n",
    "for t in list(zero_counts_respondent_wins):\n",
    "    if t not in list(zero_counts_petitioner_wins):\n",
    "        token_only_present_when_petitioner_wins[t] = petitioner_wins.iloc[-1][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to DF\n",
    "token_only_present_when_respondent_wins = pd.DataFrame(token_only_present_when_respondent_wins, \n",
    "                                                       index = [0])\n",
    "token_only_present_when_petitioner_wins = pd.DataFrame(token_only_present_when_petitioner_wins,\n",
    "                                                       index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most common tokens used when the respondent wins but not present when the petitioner wins and vice versa\n",
    "most_common_token_only_present_when_respondent_wins = token_only_present_when_respondent_wins.iloc[0].nlargest(10)\n",
    "most_common_token_only_present_when_petitioner_wins = token_only_present_when_petitioner_wins.iloc[0].nlargest(10)\n",
    "print(\"Most common tokens only present when petitioner wins: \\n\", most_common_token_only_present_when_respondent_wins,\n",
    "      sep = '')\n",
    "print(\"-\" * 100)\n",
    "print(\"Most common tokens only present when respondent wins: \\n\", most_common_token_only_present_when_petitioner_wins,\n",
    "      sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
