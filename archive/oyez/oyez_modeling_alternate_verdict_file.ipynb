{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unzip and read turns files into pandas df\n",
    "zf1 = zipfile.ZipFile('./turns_part1.zip') \n",
    "turns1 = pd.read_csv(zf1.open('turns_part1.csv'))\n",
    "zf2 = zipfile.ZipFile('./turns_part2.zip') \n",
    "turns2 = pd.read_csv(zf2.open('turns_part2.csv'))\n",
    "zf3 = zipfile.ZipFile('./turns_part3.zip') \n",
    "turns3 = pd.read_csv(zf3.open('turns_part3.csv'))\n",
    "zf4 = zipfile.ZipFile('./turns_part4.zip') \n",
    "turns4 = pd.read_csv(zf4.open('turns_part4.csv'))\n",
    "\n",
    "#read summaries file into pandas df\n",
    "#summaries = pd.read_csv('summaries.csv')\n",
    "summaries = pd.read_csv(\"[Not_Oyez]SCDB_2017_01_caseCentered_Citation.csv\", encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concate the turns files\n",
    "turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "#remove _t01 and _t02 from transcript_id in turns_combined\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create transcript_id by concatenating year and docket\n",
    "summaries['transcript_id'] = summaries.apply(lambda x: str(x['term']) + \"_\" + str(x['docket']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pivot turns files by transcript_id and speaker_role for texts\n",
    "speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "                                                     columns = 'speaker_role', \n",
    "                                                     values = 'text',\n",
    "                                                     aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "#reset index\n",
    "speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "#join verdict into df\n",
    "train_test_df = speaker_role_text_pivot.join(summaries.set_index('transcript_id')['decisionType'], on='transcript_id')\n",
    "\n",
    "#remove NAs and blanks (these give errors when vectorizing)\n",
    "train_test_df = train_test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create train and test split\n",
    "x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "y = train_test_df.decisionType\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "                                        columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "                                        columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "                                        columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "                                        columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "#concatenate the not_a_justice bow and scotus_justice bow\n",
    "x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.8198\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.6832\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.7624\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "random_forest_prediction = forest.predict(x_test)\n",
    "rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# Multinomial Naive Baynes\n",
    "nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "naive_baynes_prediction = nb.predict(x_test)\n",
    "nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# logistic regression\n",
    "lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "lr_model_train = lr_model.fit(x_train, y_train)\n",
    "lr_prediction = lr_model.predict(x_test)\n",
    "lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petitioner Count: 504\n",
      "Respondent Count: 1\n"
     ]
    }
   ],
   "source": [
    "petitioner_count = 0\n",
    "respondent_count = 0\n",
    "others = 0\n",
    "for prediction in list(forest.predict(x_test)):\n",
    "    if prediction == 1.0:\n",
    "        petitioner_count += 1\n",
    "    else:\n",
    "        respondent_count += 1\n",
    "        \n",
    "print(\"Petitioner Count:\", petitioner_count)\n",
    "print(\"Respondent Count:\", respondent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bag-of-Words Random Forest Model: 0.8402\n",
      "Accuracy of Bag-of-Words Multinomial Naive Baynes Model: 0.7972\n",
      "Accuracy of Bag-of-Words Logistic Regression Model: 0.8851\n"
     ]
    }
   ],
   "source": [
    "### For Use With Oyez Summaries File\n",
    "\n",
    "# #remove OA and _orig from transcript id in summaries df\n",
    "# summaries['transcript_id'] = summaries['transcript_id'].str.replace('OA','')\n",
    "# summaries['transcript_id'] = summaries['transcript_id'].str.replace('_orig','')\n",
    "\n",
    "# #create new column for verdict variable\n",
    "# def f(row):\n",
    "#     if row['winning_party'] == row['first_party']:\n",
    "#         val = row['first_party_label']\n",
    "#     elif row['winning_party'] == row['second_party']:\n",
    "#         val = row['second_party_label']\n",
    "#     else:\n",
    "#         val = 'No Verdict'\n",
    "#     return val\n",
    "\n",
    "# summaries['verdict'] = summaries.apply(f, axis=1)\n",
    "\n",
    "# #concate the turns files\n",
    "# turns_combined = pd.concat([turns1, turns2, turns3, turns4])\n",
    "\n",
    "# #remove _t01 and _t02 from transcript_id in turns_combined\n",
    "# turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t01','')\n",
    "# turns_combined['transcript_id'] = turns_combined['transcript_id'].str.replace('_t02','')\n",
    "\n",
    "# #pivot turns files by transcript_id and speaker_role for texts\n",
    "# speaker_role_text_pivot = turns_combined.pivot_table(index = 'transcript_id', \n",
    "#                                                      columns = 'speaker_role', \n",
    "#                                                      values = 'text',\n",
    "#                                                      aggfunc=lambda x: ' '.join(x))\n",
    "\n",
    "# #reset index\n",
    "# speaker_role_text_pivot = speaker_role_text_pivot.reset_index()\n",
    "\n",
    "# #join verdict into df\n",
    "# train_test_df = speaker_role_text_pivot.join(summaries.set_index('transcript_id')['verdict'], on='transcript_id')\n",
    "\n",
    "# #remove NAs and blanks (these give errors when vectorizing)\n",
    "# train_test_df = train_test_df[train_test_df.verdict.notnull()]\n",
    "# train_test_df = train_test_df.dropna()\n",
    "# train_test_df = train_test_df[train_test_df['verdict'] != 'No Verdict']\n",
    "\n",
    "# #create train and test split\n",
    "# x = train_test_df[['not_a_justice', 'scotus_justice']]\n",
    "# y = train_test_df.verdict\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33)\n",
    "\n",
    "# #count vectorizer\n",
    "# count_vect = CountVectorizer()\n",
    "# x_train_cv_not_a_justice = count_vect.fit(x_train['not_a_justice'])\n",
    "# x_train_cv_scotus_justice = count_vect.fit(x_train['scotus_justice'])\n",
    "# x_test_cv_not_a_justice = count_vect.fit(x_test['not_a_justice'])\n",
    "# x_test_cv_scotus_justice = count_vect.fit(x_test['scotus_justice'])\n",
    "\n",
    "# x_train_cv_not_a_justice = pd.DataFrame(x_train_cv_not_a_justice.transform(x_train['not_a_justice']).todense(),\n",
    "#                                         columns = x_train_cv_not_a_justice.get_feature_names())\n",
    "# x_train_cv_scotus_justice = pd.DataFrame(x_train_cv_scotus_justice.transform(x_train['scotus_justice']).todense(),\n",
    "#                                         columns = x_train_cv_scotus_justice.get_feature_names())\n",
    "# x_test_cv_not_a_justice = pd.DataFrame(x_test_cv_not_a_justice.transform(x_test['not_a_justice']).todense(),\n",
    "#                                         columns = x_test_cv_not_a_justice.get_feature_names())\n",
    "# x_test_cv_scotus_justice = pd.DataFrame(x_test_cv_scotus_justice.transform(x_test['scotus_justice']).todense(),\n",
    "#                                         columns = x_test_cv_scotus_justice.get_feature_names())\n",
    "\n",
    "# #concatenate the not_a_justice bow and scotus_justice bow\n",
    "# x_train = pd.concat([x_train_cv_not_a_justice, x_train_cv_scotus_justice], axis = 1)\n",
    "# x_test = pd.concat([x_test_cv_not_a_justice, x_test_cv_scotus_justice], axis = 1)\n",
    "\n",
    "# #random forest\n",
    "# forest = RandomForestClassifier(n_estimators = 100)\n",
    "# forest = forest.fit(x_train, y_train)\n",
    "# random_forest_prediction = forest.predict(x_test)\n",
    "# rf_accuracy = np.mean(random_forest_prediction == y_test)\n",
    "# print('Accuracy of Bag-of-Words Random Forest Model:', round(rf_accuracy,4))\n",
    "\n",
    "# # Multinomial Naive Baynes\n",
    "# nb = MultinomialNB(alpha = 0.1).fit(x_train, y_train)\n",
    "# naive_baynes_prediction = nb.predict(x_test)\n",
    "# nb_accuracy = np.mean(naive_baynes_prediction == y_test)\n",
    "# print('Accuracy of Bag-of-Words Multinomial Naive Baynes Model:', round(nb_accuracy,4))\n",
    "\n",
    "# # logistic regression\n",
    "# lr_model = LogisticRegression(C = 1, penalty = \"l2\")\n",
    "# lr_model_train = lr_model.fit(x_train, y_train)\n",
    "# lr_prediction = lr_model.predict(x_test)\n",
    "# lr_accuracy = np.mean(lr_prediction == y_test)\n",
    "# print('Accuracy of Bag-of-Words Logistic Regression Model:', round(lr_accuracy,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
